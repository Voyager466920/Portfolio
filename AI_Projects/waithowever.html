<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Wait, However</title>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;700&display=swap" rel="stylesheet">
    <link rel="icon" href="../images/telescope.png" type="image/png">
    <link rel="stylesheet" href="ai_projects.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
</head>
<body>
    <div class="top-menu">
        <a href="../index.html">Junha</a>
        <a href="../ai_development.html">AI</a>
        <a href="../app_development.html">App</a>
        <a href="../contact.html">Contact</a>
        <i id="darkModeToggle" class="fas fa-moon"></i>
    </div>
  <section class="project-page">
    <a 
      href="https://github.com/Voyager466920/BFG_2B" 
      target="_blank" 
      rel="noopener noreferrer"
      class="sticker-link">
      <img src="../images/sticker/github.png" alt="" class="sticker"
          style="top: 8%; right: 20%;">
    </a>
    
    <div class="project-content">
      <h1 class="project-title">Wait, However</h1>
      <p class="subtitle">Mounting LLM on iOS app.</p>

      <p class="description">
        I only used a single Nvidia RTX 3090 GPU to pretrain a 150M Korean Language Model — fully from scratch.
        Built a tokenizer with SentencePiece, using LatentMoE architecture, gathering datasets and finetuned.
        Open-source on GitHub and Hugging Face.
      </p>
      <p class="description">
        There were still many failures throughout this project. To begin with, the computer died. 
        I pushed it far beyond its limit. The main issue was with the power supply. 
        Ironically, I had even written code to pause every 200 steps just to cool things down, but it worsen the problem.
        So, technically I did stress test on my computer. 
        But the problem didn't stop at hardware. There were plenty of software issues, too.
        Let me list them out. 
        <ul class="indented-list">
          <li>Each epoch took about 36 hours.</li>
          <li>The dataset was too small to train effectively</li>
          <li>The system temperature kept rising despite cooldown intervals</li>
          <li>Fine-tuning failed — I thought uploading to Hugging Face would help,
          but transformers didn’t support LatentMoE properly</li>
          <li>Compatibility issues with huggingface-hub tools made it even harder</li>
        </ul>
        In the end, it wasn’t just the machine that broke —I was the one who got pushed to the limits.<br><br>

        I first followed Deepseek's module and added MoE architecture. Back then, there was no R2 which Deepseek applied the MoE architecture.
        It is built from scratch and you can find the architecture from my Github.
        I first pretrained with Korean Wiki and AI hub dataset which is consisted of mainly news.
        I used SentencePiece to make a tokenizer but, one thought pass through my mind.
        Korean is super easy to learn but has a massive learning curve. This is because unlike English, Korean has diverse morpheme.
        So, I decided to make a tokenizer out of morpheme, using Mecab and SentencePiece.
        Advantage of using morpheme as a tokenizer is first, the model can truely catch the pattern of the grammar of language and can increase the size of the datasets.
      </p>

      <picture class="project-img">
        <img src="../AI_Projects/KoRaptor_Images/ppl_graph.png" alt="Inference Result" />
      </picture>
    </div>

    <div class="project-content">
      <p class="description">
        I stopped at 5th epoch for pretraining for two reasons. First, validation perplexity started increasing after the 4th epoch, peaking by the 6th.
        Since perplexity is the exponential of the loss, this indicated that the loss was also increasing. 
        Second, each epoch required around 36 hours to complete.
        Given that the 4th epoch showed the best validation perplexity, I decided to proceed with fine-tuning this model as a chatbot.<br><br>
        (Although I mentioned it breifly, this stage involved a lot of trial and error. Initially, I attempted to train a 1B model first, 
        but it couldn't run on my GPU efficiently. So changed to 72M, read Chinchilla Scaling Law paper, found the optimal model size that can be pretrained. This took about a month.)

      </p>
    </div>

      <picture class="project-img">
        <source srcset="../AI_Projects/KoRaptor_Images/inference_mobile.png" media="(max-width: 768px)" />
        <img src="../AI_Projects/KoRaptor_Images/inference.png" alt="Inference Result" />
      </picture>
    </div>
    

    <div class="project-content">
      <p class="description">
        As you can see from the image, the Next Token Prediction is actually not bad.
        For those who don't understand korean, my prompt was "The president has".
        Raptor model predicted the next sentence as "promised that 'Through this ceremeony, we will do our best to protect precious lives living in our community.'".
        You can tell that the model understood how korean works. 
      </p>
    </div>
  </section>


  <script>
        // 다크 모드 토글
        const darkModeToggle = document.getElementById('darkModeToggle');
        let darkModeEnabled = false;

        if (localStorage.getItem('darkMode') === 'enabled') {
            darkModeEnabled = true;
            document.body.classList.add('dark-mode');
            darkModeToggle.classList.remove('fa-moon');
            darkModeToggle.classList.add('fa-lightbulb');
        }

        darkModeToggle.addEventListener('click', () => {
            darkModeEnabled = !darkModeEnabled;
            document.body.classList.toggle('dark-mode', darkModeEnabled);

            if (darkModeEnabled) {
                darkModeToggle.classList.remove('fa-moon');
                darkModeToggle.classList.add('fa-lightbulb');
                localStorage.setItem('darkMode', 'enabled');
            } else {
                darkModeToggle.classList.remove('fa-lightbulb');
                darkModeToggle.classList.add('fa-moon');
                localStorage.setItem('darkMode', 'disabled');
            }
        });

        const navLinks = document.querySelectorAll('.top-menu a');

        navLinks.forEach(link => {
            link.addEventListener('click', (event) => {
                event.preventDefault(); 
                const targetUrl = link.getAttribute('href');
                location.replace(targetUrl);
            });
        });

    </script>
</body>
</html>
