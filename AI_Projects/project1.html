<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Project 1</title>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;700&display=swap" rel="stylesheet">
    <link rel="icon" href="../images/telescope.png" type="image/png">
    <link rel="stylesheet" href="project.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
</head>
<body>
    <div class="top-menu">
        <a href="../index.html">Junha</a>
        <a href="../ai_development.html">AI</a>
        <a href="../app_development.html">App</a>
        <a href="../contact.html">Contact</a>
        <i id="darkModeToggle" class="fas fa-moon"></i>
    </div>
    <div class="container">
        <header>
            <h1 class="title">Help! I Accidently Built ChatGPT from Scratch.</h1>
            <div class="author-info">
                <span class="author">By Junha Park</span>
                <span class="date">Published on Nov 9, 2023</span>
            </div>
        </header>
        <main>
            <div class="image-container">
                <img src="../blog_images/chatGPT1.jpg" alt="Help I accidentally...">
            </div>
            <section class="paragraph">
                <h2>Overview</h2>
                <p>
                    “If in doubt, code it out.” This phrase comes from my online teacher, Daniel Bourke, who taught me how to build AI models. He emphasizes the importance of visualization, because artificial intelligence is essentially a black box. You don’t really know what’s going on inside. It’s packed with math, weights and biases, gradient descent, loss functions, and more. These concepts can feel intimidating at first.
                    But I believe the best way to learn something is to dive straight into building a system. Everyone learns differently, but for me, jumping into the deep end definitely worked. That’s why, when I got curious about GPT, I decided to try building one myself.
                </p>
            </section>

            <section class="paragraph">
                <h2>Architecture</h2>
                <div class="image-container">
                    <img src="../blog_images/chatGPT2.png" alt="Transformer Architecture">
                </div>
                <p>
                    As you can see in Figure A (from the original paper), BERT is built by stacking multiple encoders. That’s why BERT isn’t used for generating text. GPT, however, is built by stacking decoders — about 12 layers in the case of GPT-2 — so it can generate text. If a model uses both encoder and decoder, it’s called a T5 (Text-to-Text Transfer Transformer) model. Since we’re building a GPT-like model, we’ll focus on the decoder only.
                    In Figure A, the decoder has multiple layers that follow a specific structure: Masked Multi-Head Attention → Add & Norm → Multi-Head Attention → Add & Norm → Residual Connection → FeedForward → Add & Norm. This process is repeated N times, and the final result is passed through a Linear layer and a Softmax function.
                </p>
            </section>

            <section class="paragraph">
                <h2>Tokenization</h2>
                <p>
                    Tokenization is crutial for LLM. LLM, a Large Language Model doesn’t actually read or understand the context of sentences. Humans read the line just by looking at the sentence level. Actually, human don’t look all the words inside the sentence. You assume what inner word will be. On the other hand, computers looks at the words. You can learn by Computer Vision sector for example making CNN or ViT. AI model breaks image into smaller patch of images and analyzes the RGB value. In the same way, sentence needs to be patched, we call it tokenization. I’ve used KR-Bert Tokenizer and Kkma to tokenize the korean sentence. Kkma is a tool that cuts the sentence into smaller morpheme. Korean morpheme is more complex than that of English or other words. Also, I’ve tested with just using KR-Bert Tokenizer and using both and got 1%~2% better quality.
                </p>
            </section>
        </main>
    </div>
    <script>
        // 다크 모드 토글
        const darkModeToggle = document.getElementById('darkModeToggle');
        let darkModeEnabled = false;

        if (localStorage.getItem('darkMode') === 'enabled') {
            darkModeEnabled = true;
            document.body.classList.add('dark-mode');
            darkModeToggle.classList.remove('fa-moon');
            darkModeToggle.classList.add('fa-lightbulb');
        }

        darkModeToggle.addEventListener('click', () => {
            darkModeEnabled = !darkModeEnabled;
            document.body.classList.toggle('dark-mode', darkModeEnabled);

            if (darkModeEnabled) {
                darkModeToggle.classList.remove('fa-moon');
                darkModeToggle.classList.add('fa-lightbulb');
                localStorage.setItem('darkMode', 'enabled');
            } else {
                darkModeToggle.classList.remove('fa-lightbulb');
                darkModeToggle.classList.add('fa-moon');
                localStorage.setItem('darkMode', 'disabled');
            }
        });

        const navLinks = document.querySelectorAll('.top-menu a');

        navLinks.forEach(link => {
            link.addEventListener('click', (event) => {
                event.preventDefault(); 
                const targetUrl = link.getAttribute('href');
                location.replace(targetUrl);
            });
        });

    </script>
    
</body>
</html>
